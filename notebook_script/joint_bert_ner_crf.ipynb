{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    def __init__(self):\n",
    "        self.dataset='DuEE1.0'\n",
    "        self.event_type='trigger'\n",
    "        self.max_len=200\n",
    "        self.per_gpu_train_batch_size=16\n",
    "        self.per_gpu_eval_batch_size=32\n",
    "        #self.model_name_or_path='F:/prev_trained_model/chinese_wwm_pytorch'\n",
    "        self.model_name_or_path='F:/prev_trained_model/rbt3'\n",
    "        self.linear_learning_rate=1e-4\n",
    "        self.early_stop=5\n",
    "        self.seed=1\n",
    "        self.output_dir='../output'\n",
    "        self.num_train_epochs=50\n",
    "        self.weight_decay=0.01\n",
    "        self.learning_rate=1e-5\n",
    "        self.adam_epsilon=1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast, AdamW\n",
    "\n",
    "from dataset.dataset import joint_collate_fn,DuEEJointDataset\n",
    "from metric.metric import ChunkEvaluator\n",
    "from model.model import DuEEEvent_model\n",
    "from utils.finetuning_argparse import get_argparse\n",
    "from utils.utils import init_logger, seed_everything, logger, ProgressBar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_argparse().parse_args()\n",
    "    print(json.dumps(vars(args), sort_keys=True, indent=4, separators=(', ', ': '), ensure_ascii=False))\n",
    "    init_logger(log_file=\"./log/{}.log\".format(time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())))\n",
    "    seed_everything(args.seed)\n",
    "\n",
    "    args.output_model_path = os.path.join(args.output_dir, args.dataset, args.event_type, \"best_model.pkl\")\n",
    "    # 设置保存目录\n",
    "    if not os.path.exists(os.path.dirname(args.output_model_path)):\n",
    "        os.makedirs(os.path.dirname(args.output_model_path))\n",
    "\n",
    "    # device\n",
    "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(args.model_name_or_path)\n",
    "\n",
    "    # dataset & dataloader\n",
    "    args.train_data = \"./data/{}/{}/train.tsv\".format(args.dataset, args.event_type)\n",
    "    args.dev_data = \"./data/{}/{}/dev.tsv\".format(args.dataset, args.event_type)\n",
    "    args.tag_path = \"./conf/{}/{}_tag.dict\".format(args.dataset, args.event_type)\n",
    "    train_dataset = DuEEEventDataset(args,\n",
    "                                     args.train_data,\n",
    "                                     args.tag_path,\n",
    "                                     tokenizer)\n",
    "    eval_dataset = DuEEEventDataset(args,\n",
    "                                    args.dev_data,\n",
    "                                    args.tag_path,\n",
    "                                    tokenizer)\n",
    "    logger.info(\"The nums of the train_dataset features is {}\".format(len(train_dataset)))\n",
    "    logger.info(\"The nums of the eval_dataset features is {}\".format(len(eval_dataset)))\n",
    "    train_iter = DataLoader(train_dataset,\n",
    "                            shuffle=True,\n",
    "                            batch_size=args.per_gpu_train_batch_size,\n",
    "                            collate_fn=collate_fn,\n",
    "                            num_workers=20)\n",
    "    eval_iter = DataLoader(eval_dataset,\n",
    "                           shuffle=False,\n",
    "                           batch_size=args.per_gpu_eval_batch_size,\n",
    "                           collate_fn=collate_fn,\n",
    "                           num_workers=20)\n",
    "\n",
    "    # 用于evaluate\n",
    "    args.id2label = train_dataset.label_vocab\n",
    "    args.num_classes = len(args.id2label)\n",
    "    metric = ChunkEvaluator(label_list=args.id2label.keys(), suffix=False)\n",
    "\n",
    "    # model\n",
    "    model = DuEEEvent_model(args.model_name_or_path, num_classes=args.num_classes)\n",
    "    model.to(args.device)\n",
    "\n",
    "    best_f1 = 0\n",
    "    early_stop = 0\n",
    "    for epoch, _ in enumerate(range(int(args.num_train_epochs))):\n",
    "        model.train()\n",
    "        train(args, train_iter, model)\n",
    "        eval_p, eval_r, eval_f1, eval_loss = evaluate(args, eval_iter, model, metric)\n",
    "        logger.info(\n",
    "            \"The F1-score is {}\".format(eval_f1)\n",
    "        )\n",
    "        if eval_f1 > best_f1:\n",
    "            early_stop = 0\n",
    "            best_f1 = eval_f1\n",
    "            logger.info(\"the best eval f1 is {:.4f}, saving model !!\".format(best_f1))\n",
    "            best_model = copy.deepcopy(model.module if hasattr(model, \"module\") else model)\n",
    "            torch.save(best_model.state_dict(), args.output_model_path)\n",
    "        else:\n",
    "            early_stop += 1\n",
    "            if early_stop == args.early_stop:\n",
    "                logger.info(\"Early stop in {} epoch!\".format(epoch))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"adam_epsilon\": 1e-08, \n",
      "    \"dataset\": \"DuEE1.0\", \n",
      "    \"early_stop\": 5, \n",
      "    \"event_type\": \"trigger\", \n",
      "    \"learning_rate\": 1e-05, \n",
      "    \"linear_learning_rate\": 0.0001, \n",
      "    \"max_len\": 200, \n",
      "    \"model_name_or_path\": \"F:/prev_trained_model/rbt3\", \n",
      "    \"num_train_epochs\": 50, \n",
      "    \"output_dir\": \"../output\", \n",
      "    \"per_gpu_eval_batch_size\": 32, \n",
      "    \"per_gpu_train_batch_size\": 16, \n",
      "    \"seed\": 1, \n",
      "    \"weight_decay\": 0.01\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing...: 100%|███████████████████████████████████████████████████████████| 11958/11958 [00:07<00:00, 1652.03it/s]\n",
      "tokenizing...: 100%|███████████████████████████████████████████████████████████| 13915/13915 [00:08<00:00, 1557.55it/s]\n",
      "tokenizing...: 100%|█████████████████████████████████████████████████████████████| 1498/1498 [00:00<00:00, 1638.95it/s]\n",
      "tokenizing...: 100%|█████████████████████████████████████████████████████████████| 1790/1790 [00:01<00:00, 1495.00it/s]\n",
      "05/18/2021 10:09:11 - INFO - root -   The nums of the train_dataset features is 13915\n",
      "05/18/2021 10:09:11 - INFO - root -   The nums of the eval_dataset features is 1790\n"
     ]
    }
   ],
   "source": [
    "args=CFG()\n",
    "\n",
    "print(json.dumps(vars(args), sort_keys=True, indent=4, separators=(', ', ': '), ensure_ascii=False))\n",
    "init_logger(log_file=\".././log/{}.log\".format(time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())))\n",
    "seed_everything(args.seed)\n",
    "\n",
    "args.output_model_path = os.path.join(args.output_dir, args.dataset, args.event_type, \"best_model.pkl\")\n",
    "# 设置保存目录\n",
    "if not os.path.exists(os.path.dirname(args.output_model_path)):\n",
    "    os.makedirs(os.path.dirname(args.output_model_path))\n",
    "\n",
    "# device\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(args.model_name_or_path)\n",
    "\n",
    "# dataset & dataloader\n",
    "# args.train_data = \"../data/{}/{}/train.tsv\".format(args.dataset, args.event_type)\n",
    "# args.dev_data = \"../data/{}/{}/dev.tsv\".format(args.dataset, args.event_type)\n",
    "# args.tag_path = \"../conf/{}/{}_tag.dict\".format(args.dataset, args.event_type)\n",
    "\n",
    "#trigger dataset\n",
    "args.trigger_train_data = \"../data/DuEE1.0/trigger/train.tsv\"\n",
    "args.trigger_dev_data = \"../data/DuEE1.0/trigger/dev.tsv\"\n",
    "args.trigger_tag_path = \"../conf/DuEE1.0/trigger_tag.dict\"\n",
    "\n",
    "#role dataset\n",
    "args.role_train_data = \"../data/DuEE1.0/role/train.tsv\"\n",
    "args.role_dev_data = \"../data/DuEE1.0/role/dev.tsv\"\n",
    "args.role_tag_path = \"../conf/DuEE1.0/role_tag.dict\"\n",
    "\n",
    "train_dataset = DuEEJointDataset(args,\n",
    "                                 args.trigger_train_data,\n",
    "                                 args.role_train_data,\n",
    "                                 args.trigger_tag_path,\n",
    "                                 args.role_tag_path,\n",
    "                                 tokenizer)\n",
    "\n",
    "eval_dataset = DuEEJointDataset(args,\n",
    "                                args.trigger_dev_data,\n",
    "                                args.role_dev_data,\n",
    "                                args.trigger_tag_path,\n",
    "                                args.role_tag_path,\n",
    "                                tokenizer)\n",
    "\n",
    "logger.info(\"The nums of the train_dataset features is {}\".format(len(train_dataset)))\n",
    "logger.info(\"The nums of the eval_dataset features is {}\".format(len(eval_dataset)))\n",
    "train_iter = DataLoader(train_dataset,\n",
    "                        shuffle=True,\n",
    "                        batch_size=args.per_gpu_train_batch_size,\n",
    "                        collate_fn=joint_collate_fn,\n",
    "                        num_workers=0)\n",
    "eval_iter = DataLoader(eval_dataset,\n",
    "                       shuffle=False,\n",
    "                       batch_size=args.per_gpu_eval_batch_size,\n",
    "                       collate_fn=joint_collate_fn,\n",
    "                       num_workers=0)\n",
    "\n",
    "# 用于evaluate\n",
    "args.tagger_id2label = train_dataset.tagger_dataset.label_vocab\n",
    "args.num_tagger_classes = len(args.tagger_id2label)\n",
    "tagegr_metric = ChunkEvaluator(label_list=args.tagger_id2label.keys(), suffix=False)\n",
    "\n",
    "args.role_id2label = train_dataset.role_dataset.label_vocab\n",
    "args.num_role_classes = len(args.role_id2label)\n",
    "role_metric = ChunkEvaluator(label_list=args.role_id2label.keys(), suffix=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, eval_iter, model, tagger_metric,role_metric):\n",
    "    \"\"\"evaluate\"\"\"\n",
    "    tagger_metric.reset()\n",
    "    role_metric.reset()\n",
    "    batch_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1).to(args.device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(eval_iter):\n",
    "            \n",
    "            for key in batch.keys():\n",
    "                batch[key] = batch[key].to(args.device)\n",
    "            tagger_logits,role_logits = model(\n",
    "            input_ids=batch['all_input_ids'],\n",
    "            attention_mask=batch['all_attention_mask'],\n",
    "            token_type_ids=batch['all_token_type_ids'],\n",
    "            )\n",
    "            #loss = criterion(logits.view(-1, args.num_classes),batch[\"all_labels\"].view(-1))\n",
    "            #batch_loss += loss.item()\n",
    "\n",
    "            \n",
    "            #tagger_preds = torch.argmax(tagger_logits, axis=-1)\n",
    "            tagger_preds=torch.tensor(model.tagger_crf.decode(tagger_logits),dtype=torch.int)\n",
    "            n_infer, n_label, n_correct = tagger_metric.compute(batch[\"all_seq_lens\"], tagger_preds, batch['all_tagger_labels'])\n",
    "            tagger_metric.update(n_infer, n_label, n_correct)\n",
    "            \n",
    "            #role_preds = torch.argmax(role_logits, axis=-1)\n",
    "            role_preds=torch.tensor(model.role_crf.decode(role_logits),dtype=torch.int)\n",
    "            n_infer, n_label, n_correct = role_metric.compute(batch[\"all_seq_lens\"], role_preds, batch['all_role_labels'])\n",
    "            role_metric.update(n_infer, n_label, n_correct)\n",
    "            \n",
    "    precision1, recall1, f1_score1 = tagger_metric.accumulate()\n",
    "    precision2, recall2, f1_score2 = role_metric.accumulate()\n",
    "\n",
    "    return precision1, recall1, f1_score1,precision2, recall2, f1_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "class DuEEEvent_joint_crf_model(nn.Module):\n",
    "    def __init__(self, pretrained_model_path, num_tagger_classes,num_role_class):\n",
    "        super(DuEEEvent_joint_crf_model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_path)\n",
    "        self.tagger_classifier = nn.Linear(self.bert.config.hidden_size, num_tagger_classes)\n",
    "        self.role_classifier = nn.Linear(self.bert.config.hidden_size, num_role_class)\n",
    "        self.tagger_crf = CRF(num_tags=num_tagger_classes, batch_first=True)\n",
    "        self.role_crf = CRF(num_tags=num_role_class, batch_first=True)\n",
    "        \n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=None,\n",
    "                tagger_labels=None,\n",
    "                role_labels=None):\n",
    "        output = self.bert(input_ids,\n",
    "                           token_type_ids=token_type_ids,\n",
    "                           attention_mask=attention_mask)\n",
    "        sequence_output, pooled_output = output[0], output[1]\n",
    "        tagger_logits = self.tagger_classifier(sequence_output)\n",
    "        role_logits=self.role_classifier(sequence_output)\n",
    "        \n",
    "        if tagger_labels is not None and role_labels is not None:\n",
    "            loss1 = self.tagger_crf(emissions=tagger_logits, tags=tagger_labels, mask=attention_mask.to(torch.uint8))\n",
    "            loss2 = self.role_crf(emissions=role_logits, tags=role_labels, mask=attention_mask.to(torch.uint8))\n",
    "            loss=loss1+loss2\n",
    "            return -1 * loss\n",
    "        return tagger_logits,role_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = DuEEEvent_joint_crf_model(args.model_name_or_path, num_tagger_classes=args.num_tagger_classes,num_role_class=args.num_role_classes)\n",
    "_=model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_iter, model):\n",
    "    logger.info(\"***** Running train *****\")\n",
    "    # 优化器\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    bert_param_optimizer = list(model.bert.named_parameters())\n",
    "    linear_param_optimizer = list(model.tagger_classifier.named_parameters())\n",
    "    \n",
    "    linear_param_optimizer.extend(list(model.role_classifier.named_parameters()))\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in bert_param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay,\n",
    "         'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in bert_param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.0,\n",
    "         'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in linear_param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay,\n",
    "         'lr': args.linear_learning_rate},\n",
    "        {'params': [p for n, p in linear_param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.0,\n",
    "         'lr': args.linear_learning_rate},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=args.learning_rate,\n",
    "                      eps=args.adam_epsilon)\n",
    "    # 损失函数\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1).to(args.device)\n",
    "    batch_loss = 0\n",
    "    pbar = ProgressBar(n_total=len(train_iter), desc='Training')\n",
    "    print(\"****\" * 20)\n",
    "    for step, batch in enumerate(train_iter):\n",
    "        for key in batch.keys():\n",
    "            batch[key] = batch[key].to(args.device)\n",
    "            \n",
    "        loss = model(\n",
    "            input_ids=batch['all_input_ids'],\n",
    "            attention_mask=batch['all_attention_mask'],\n",
    "            token_type_ids=batch['all_token_type_ids'],\n",
    "            tagger_labels=batch['all_tagger_labels'],\n",
    "            role_labels=batch['all_role_labels']\n",
    "        )\n",
    "\n",
    "        #tagger_logits = tagger_logits.view(-1, args.num_tagger_classes)\n",
    "        #role_logits = role_logits.view(-1, args.num_role_classes)\n",
    "        # 正常训练\n",
    "        #loss1 = criterion(tagger_logits, batch[\"all_tagger_labels\"].view(-1))\n",
    "        #loss2= criterion(role_logits, batch[\"all_role_labels\"].view(-1))\n",
    "        loss.backward()\n",
    "        #\n",
    "        batch_loss += loss.item()\n",
    "        pbar(step,\n",
    "             {\n",
    "                 'batch_loss': batch_loss / (step + 1),\n",
    "             })\n",
    "        optimizer.step()\n",
    "        model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 10:09:13 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 735.0ms/step  batch_loss: 1498.6948 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 10:20:36 - INFO - root -   The trigger F1-score is 0.6129032258064516 , the role F1-score is 0.2217438105489774\n",
      "05/18/2021 10:20:36 - INFO - root -   the best trigger eval f1 is 0.6129 , role eval f1 is 0.2217, saving model !!\n",
      "05/18/2021 10:20:37 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 741.3ms/step  batch_loss: 608.2956 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 10:32:05 - INFO - root -   The trigger F1-score is 0.7349353049907579 , the role F1-score is 0.35256828563438053\n",
      "05/18/2021 10:32:05 - INFO - root -   the best trigger eval f1 is 0.7349 , role eval f1 is 0.3526, saving model !!\n",
      "05/18/2021 10:32:06 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 737.2ms/step  batch_loss: 436.6978 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 10:43:32 - INFO - root -   The trigger F1-score is 0.7666666666666666 , the role F1-score is 0.4149867644064345\n",
      "05/18/2021 10:43:32 - INFO - root -   the best trigger eval f1 is 0.7667 , role eval f1 is 0.4150, saving model !!\n",
      "05/18/2021 10:43:32 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 744.2ms/step  batch_loss: 368.8011 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 10:55:03 - INFO - root -   The trigger F1-score is 0.7826242402574187 , the role F1-score is 0.44004190875321464\n",
      "05/18/2021 10:55:03 - INFO - root -   the best trigger eval f1 is 0.7826 , role eval f1 is 0.4400, saving model !!\n",
      "05/18/2021 10:55:04 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 737.4ms/step  batch_loss: 326.9737 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 11:06:30 - INFO - root -   The trigger F1-score is 0.7838827838827838 , the role F1-score is 0.45730769230769225\n",
      "05/18/2021 11:06:30 - INFO - root -   the best trigger eval f1 is 0.7839 , role eval f1 is 0.4573, saving model !!\n",
      "05/18/2021 11:06:30 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 753.9ms/step  batch_loss: 296.0407 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 11:18:10 - INFO - root -   The trigger F1-score is 0.7879661322284273 , the role F1-score is 0.483814352574103\n",
      "05/18/2021 11:18:10 - INFO - root -   the best trigger eval f1 is 0.7880 , role eval f1 is 0.4838, saving model !!\n",
      "05/18/2021 11:18:11 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 744.4ms/step  batch_loss: 271.6236 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 11:29:42 - INFO - root -   The trigger F1-score is 0.7966697502312674 , the role F1-score is 0.47279159296639267\n",
      "05/18/2021 11:29:42 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 744.5ms/step  batch_loss: 252.3718 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 11:41:13 - INFO - root -   The trigger F1-score is 0.7906215921483096 , the role F1-score is 0.5000479892504078\n",
      "05/18/2021 11:41:13 - INFO - root -   the best trigger eval f1 is 0.7906 , role eval f1 is 0.5000, saving model !!\n",
      "05/18/2021 11:41:14 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 748.2ms/step  batch_loss: 234.6005 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 11:52:48 - INFO - root -   The trigger F1-score is 0.7849182763744428 , the role F1-score is 0.49811089553762894\n",
      "05/18/2021 11:52:48 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 743.5ms/step  batch_loss: 219.4806 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 12:04:18 - INFO - root -   The trigger F1-score is 0.7967123287671233 , the role F1-score is 0.5041723267999223\n",
      "05/18/2021 12:04:18 - INFO - root -   the best trigger eval f1 is 0.7967 , role eval f1 is 0.5042, saving model !!\n",
      "05/18/2021 12:04:19 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 742.9ms/step  batch_loss: 207.4352 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 12:15:49 - INFO - root -   The trigger F1-score is 0.7989940722112449 , the role F1-score is 0.4958516349438751\n",
      "05/18/2021 12:15:49 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 739.3ms/step  batch_loss: 193.5903 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 12:27:15 - INFO - root -   The trigger F1-score is 0.7942951179374657 , the role F1-score is 0.49816559246405556\n",
      "05/18/2021 12:27:15 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 731.8ms/step  batch_loss: 185.3451 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 12:38:35 - INFO - root -   The trigger F1-score is 0.7963130309054762 , the role F1-score is 0.4998040752351097\n",
      "05/18/2021 12:38:35 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 735.7ms/step  batch_loss: 173.9219 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 12:49:59 - INFO - root -   The trigger F1-score is 0.7996316758747699 , the role F1-score is 0.5058041472368291\n",
      "05/18/2021 12:49:59 - INFO - root -   the best trigger eval f1 is 0.7996 , role eval f1 is 0.5058, saving model !!\n",
      "05/18/2021 12:49:59 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 731.2ms/step  batch_loss: 166.2663 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 13:01:18 - INFO - root -   The trigger F1-score is 0.7959257911967987 , the role F1-score is 0.5111734545806327\n",
      "05/18/2021 13:01:18 - INFO - root -   the best trigger eval f1 is 0.7959 , role eval f1 is 0.5112, saving model !!\n",
      "05/18/2021 13:01:19 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 740.8ms/step  batch_loss: 159.7850 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 13:12:47 - INFO - root -   The trigger F1-score is 0.7959372114496769 , the role F1-score is 0.5103787506148549\n",
      "05/18/2021 13:12:47 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 741.0ms/step  batch_loss: 151.1705 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 13:24:14 - INFO - root -   The trigger F1-score is 0.8024736267733722 , the role F1-score is 0.5248761149653123\n",
      "05/18/2021 13:24:14 - INFO - root -   the best trigger eval f1 is 0.8025 , role eval f1 is 0.5249, saving model !!\n",
      "05/18/2021 13:24:15 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 740.2ms/step  batch_loss: 144.4887 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 13:35:42 - INFO - root -   The trigger F1-score is 0.8002915451895044 , the role F1-score is 0.5148956817079088\n",
      "05/18/2021 13:35:42 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 748.8ms/step  batch_loss: 139.6216 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 13:47:17 - INFO - root -   The trigger F1-score is 0.7962827988338191 , the role F1-score is 0.5138440465707855\n",
      "05/18/2021 13:47:17 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 759.0ms/step  batch_loss: 133.5189 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 13:59:02 - INFO - root -   The trigger F1-score is 0.8008890535284312 , the role F1-score is 0.5215433425442637\n",
      "05/18/2021 13:59:02 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 746.1ms/step  batch_loss: 129.6928 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 14:10:35 - INFO - root -   The trigger F1-score is 0.7997013253686766 , the role F1-score is 0.5173013725111153\n",
      "05/18/2021 14:10:35 - INFO - root -   ***** Running train *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[Training] 870/870 [==============================] 748.7ms/step  batch_loss: 125.3887 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 14:22:10 - INFO - root -   The trigger F1-score is 0.7972495818621075 , the role F1-score is 0.5175233644859814\n",
      "05/18/2021 14:22:10 - INFO - root -   Early stop in 21 epoch!\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "early_stop = 0\n",
    "for epoch, _ in enumerate(range(int(args.num_train_epochs))):\n",
    "    model.train()\n",
    "    train(args, train_iter, model)\n",
    "    eval_p, eval_r, eval_f1, eval_p2, eval_r2, eval_f12 = evaluate(args, eval_iter, model,tagegr_metric,role_metric)\n",
    "    logger.info(\n",
    "        \"The trigger F1-score is {} , the role F1-score is {}\".format(eval_f1,eval_f12)\n",
    "    )\n",
    "    \n",
    "    sumf1=eval_f1+eval_f12\n",
    "    if sumf1 > best_f1:\n",
    "        early_stop = 0\n",
    "        best_f1 = sumf1\n",
    "        logger.info(\"the best trigger eval f1 is {:.4f} , role eval f1 is {:.4f}, saving model !!\".format(eval_f1,eval_f12))\n",
    "        best_model = copy.deepcopy(model.module if hasattr(model, \"module\") else model)\n",
    "        torch.save(best_model.state_dict(), args.output_model_path)\n",
    "    else:\n",
    "        early_stop += 1\n",
    "        if early_stop == args.early_stop:\n",
    "            logger.info(\"Early stop in {} epoch!\".format(epoch))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
